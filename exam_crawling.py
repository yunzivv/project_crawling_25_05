import requests
from bs4 import BeautifulSoup
import pandas as pd
from openpyxl import load_workbook
import time
import os
import re
import requests

# ocr-env_exam\Scripts\activate
# python exam_crawling.py

# # ê²Œì‹œíŒ ë§í¬ ê°€ì ¸ì˜¤ê¸°
# # ì›¹ í˜ì´ì§€ ìš”ì²­
# url = 'https://www.comcbt.com/xe/anne'
# response = requests.get(url)
# soup = BeautifulSoup(response.text, 'html.parser')

# # CSS ì„ íƒìì— ë§ëŠ” ë§í¬ë“¤ ì¶”ì¶œ
# links = soup.select('#gnb > ul > li:nth-child(2) > ul > li:nth-child(6) > ul li > a')

# # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ íŒŒì‹±
# data = []
# for link in links:
#     title = link.get_text(strip=True)
#     href = link.get('href')
#     full_url = requests.compat.urljoin(url, href)  # ìƒëŒ€ ê²½ë¡œ ë³´ì™„
#     data.append({'ìê²©ë“±ê¸‰': 'ì‚°ì—…ê¸°ì‚¬', 
#                  'ìê²©ì¦ëª…': title, 
#                  'href': full_url, 
#                  'ì¢…ë¥˜': 'í•„ê¸°', 
#                  'regDate': time.strftime('%Y.%m.%d - %H:%M:%S')})

# file_path = 'exam.xlsx'

# # ë°ì´í„°í”„ë ˆì„ ìƒì„±
# df_new = pd.DataFrame(data)

# # ê¸°ì¡´ íŒŒì¼ì˜ ì²« ë²ˆì§¸ ì‹œíŠ¸ì˜ ë§ˆì§€ë§‰ í–‰ ë‹¤ìŒë¶€í„° ì´ì–´ì“°ê¸°
# book = load_workbook(file_path)
# sheetname = book.sheetnames[0]
# start_row = book[sheetname].max_row

# # ê¸°ì¡´ íŒŒì¼ì— ì´ì–´ì“°ê¸°
# with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
#     df_new.to_excel(writer, sheet_name=sheetname, startrow=start_row, index=False, header=False)


# ê²Œì‹œíŒì—ì„œ hwp ë‹¤ìš´ë¡œë“œ

BASE_URL = "https://www.comcbt.com"
SAVE_DIR = "hwp_files"
os.makedirs(SAVE_DIR, exist_ok=True)

def get_post_links(board_url):
    try:
        res = requests.get(board_url)
        res.raise_for_status()
    except Exception as e:
        print(f"ê²Œì‹œíŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {board_url} - {e}")
        return []
    
    soup = BeautifulSoup(res.text, 'html.parser')
    posts = []
    for a in soup.select('td.title a[href]'):
        title = a.get_text(strip=True)
        
        # (ë³µì›ì¤‘) í¬í•¨ ì‹œ ê±´ë„ˆë›°ê¸°
        if '(ë³µì›ì¤‘)' in title:
            print(f"â­ï¸ ê±´ë„ˆëœ€ (ë³µì›ì¤‘): {title}")
            continue

        # ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ (4ìë¦¬ ìˆ«ì)
        year_match = re.search(r'(20\d{2})', title)
        if year_match:
            year = int(year_match.group(1))
            if year < 2020:
                print(f"â­ï¸ ê±´ë„ˆëœ€ (ì—°ë„ë¯¸ë‹¬): {title}")
                continue
        else:
            print(f"â­ï¸ ê±´ë„ˆëœ€ (ì—°ë„ì—†ìŒ): {title}")
            continue
        
        href = a['href']
        full_url = requests.compat.urljoin(BASE_URL, href)
        posts.append((title, full_url))
    return posts

def download_hwp_from_post(title, post_url):
    print(f"â¡ï¸ ê²Œì‹œê¸€ ì ‘ì†: {title} - {post_url}")
    try:
        res = requests.get(post_url)
        res.raise_for_status()
    except Exception as e:
        print(f"ê²Œì‹œê¸€ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {post_url} - {e}")
        return
    
    soup = BeautifulSoup(res.text, 'html.parser')
    links = soup.find_all('a')

    hwp_links = []
    for link in links:
        link_text = link.get_text(strip=True)
        if link_text.endswith('.hwp') and '(êµì‚¬ìš©)' in link_text:
            href = link.get('href')
            if href:
                hwp_links.append((link_text, href))

    if not hwp_links:
        print("  âš ï¸ '(êµì‚¬ìš©).hwp' ë§í¬ ì—†ìŒ")
        return

    link_text, href = hwp_links[0]
    file_url = requests.compat.urljoin(BASE_URL, href)
    # () ì•ê¹Œì§€ ì˜ë¼ì„œ íŒŒì¼ëª… ìƒì„±
    filename = link_text.split('(')[0].strip() + '.hwp'
    save_path = os.path.join(SAVE_DIR, filename)

    if os.path.exists(save_path):
        print(f"  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼: {filename} (ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€)")
        return

    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {link_text} - íŒŒì¼ëª…: {filename}")
    try:
        file_content = requests.get(file_url).content
        with open(save_path, 'wb') as f:
            f.write(file_content)
    except Exception as e:
        print(f"  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_url} - {e}")

def main():
    # ì—‘ì…€ì—ì„œ ê²Œì‹œíŒ URL ë¦¬ìŠ¤íŠ¸ ì½ê¸°
    df = pd.read_excel('exam_board.xlsx')
    board_urls = df['href'].tolist()

    for board_url in board_urls:
        full_board_url = requests.compat.urljoin(BASE_URL, board_url)
        print(f"\nâ–¶ ê²Œì‹œíŒ ì ‘ì†: {full_board_url}")
        posts = get_post_links(full_board_url)
        print(f"ê²Œì‹œê¸€ ê°œìˆ˜: {len(posts)}")

        for title, post_url in posts:
            download_hwp_from_post(title, post_url)

if __name__ == '__main__':
    main()
