import requests
from bs4 import BeautifulSoup
import pandas as pd
from openpyxl import load_workbook
import time

# ocr-env_exam\Scripts\activate
# python exam_crawling.py

# # ê²Œì‹œíŒ ë§í¬ ê°€ì ¸ì˜¤ê¸°
# # ì›¹ í˜ì´ì§€ ìš”ì²­
# url = 'https://www.comcbt.com/xe/anne'
# response = requests.get(url)
# soup = BeautifulSoup(response.text, 'html.parser')

# # CSS ì„ íƒìì— ë§ëŠ” ë§í¬ë“¤ ì¶”ì¶œ
# links = soup.select('#gnb > ul > li:nth-child(2) > ul > li:nth-child(6) > ul li > a')

# # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ íŒŒì‹±
# data = []
# for link in links:
#     title = link.get_text(strip=True)
#     href = link.get('href')
#     full_url = requests.compat.urljoin(url, href)  # ìƒëŒ€ ê²½ë¡œ ë³´ì™„
#     data.append({'ìê²©ë“±ê¸‰': 'ì‚°ì—…ê¸°ì‚¬', 
#                  'ìê²©ì¦ëª…': title, 
#                  'href': full_url, 
#                  'ì¢…ë¥˜': 'í•„ê¸°', 
#                  'regDate': time.strftime('%Y.%m.%d - %H:%M:%S')})

# file_path = 'exam.xlsx'

# # ë°ì´í„°í”„ë ˆì„ ìƒì„±
# df_new = pd.DataFrame(data)

# # ê¸°ì¡´ íŒŒì¼ì˜ ì²« ë²ˆì§¸ ì‹œíŠ¸ì˜ ë§ˆì§€ë§‰ í–‰ ë‹¤ìŒë¶€í„° ì´ì–´ì“°ê¸°
# book = load_workbook(file_path)
# sheetname = book.sheetnames[0]
# start_row = book[sheetname].max_row

# # ê¸°ì¡´ íŒŒì¼ì— ì´ì–´ì“°ê¸°
# with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
#     df_new.to_excel(writer, sheet_name=sheetname, startrow=start_row, index=False, header=False)


# ê²Œì‹œíŒì—ì„œ hwp ë‹¤ìš´ë¡œë“œ

import os
import re
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.comcbt.com"
BOARD_URL = "https://www.comcbt.com/xe/df"
SAVE_DIR = "hwp_files"
os.makedirs(SAVE_DIR, exist_ok=True)

def get_post_links():
    res = requests.get(BOARD_URL)
    soup = BeautifulSoup(res.text, 'html.parser')
    posts = []
    links = soup.select('td.title a[href]')
    print(f"ê²Œì‹œê¸€ ë§í¬ ê°œìˆ˜: {len(links)}")

    for a in soup.select('td.title a[href]'):
        
        title = a.get_text(strip=True)
        print(f"ë°œê²¬ëœ ì œëª©: {title}")

        # (ë³µì›ì¤‘) í¬í•¨ ì‹œ ê±´ë„ˆë›°ê¸°
        if '(ë³µì›ì¤‘)' in title:
            print(f"â­ï¸ ê±´ë„ˆëœ€ (ë³µì›ì¤‘): {title}")
            continue

        # ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ (4ìë¦¬ ìˆ«ì)
        year_match = re.search(r'(20\d{2})', title)
        if year_match:
            year = int(year_match.group(1))
            if year < 2020:
                print(f"â­ï¸ ê±´ë„ˆëœ€ (ì—°ë„ë¯¸ë‹¬): {title}")
                continue
        else:
            print(f"â­ï¸ ê±´ë„ˆëœ€ (ì—°ë„ì—†ìŒ): {title}")
            continue

        href = a['href']
        full_url = requests.compat.urljoin(BASE_URL, href)
        posts.append((title, full_url))
    return posts

def download_hwp_from_post(title, post_url):
    print(f"â¡ï¸ ê²Œì‹œê¸€ ì ‘ì†: {title} - {post_url}")
    res = requests.get(post_url)
    soup = BeautifulSoup(res.text, 'html.parser')

    links = soup.find_all('a')
    print(f"  ì „ì²´ ë§í¬ ê°œìˆ˜: {len(links)}")

    hwp_links = []
    for link in links:
        link_text = link.get_text(strip=True)
        if link_text.endswith('.hwp') and '(êµì‚¬ìš©)' in link_text:
            hwp_links.append((link_text, link.get('href')))

    print(f"  ì¡°ê±´ì— ë§ëŠ” ë§í¬ ê°œìˆ˜: {len(hwp_links)}")

    if not hwp_links:
        print("  âš ï¸ ì¡°ê±´ì— ë§ëŠ” '(êµì‚¬ìš©).hwp' ë§í¬ ì—†ìŒ")
        return

    # ì²« ë²ˆì§¸ ë§í¬ë§Œ ë‹¤ìš´ë¡œë“œ
    link_text, href = hwp_links[0]
    file_url = href  # hrefëŠ” ì´ë¯¸ ì ˆëŒ€ URLì„
    file_name = file_url.split('file_srl=')[-1] + '.hwp'  # í˜¹ì€ link_text ê·¸ëŒ€ë¡œ ì¨ë„ ë¨
    save_path = os.path.join(SAVE_DIR, file_name)

    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {link_text} - íŒŒì¼ëª…: {file_name}")
    file_content = requests.get(file_url).content
    with open(save_path, 'wb') as f:
        f.write(file_content)

posts = get_post_links()
for title, post_url in posts:
    download_hwp_from_post(title, post_url)
